{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-01T18:14:22.132218Z","iopub.status.busy":"2024-04-01T18:14:22.131779Z","iopub.status.idle":"2024-04-01T18:14:23.313874Z","shell.execute_reply":"2024-04-01T18:14:23.312751Z","shell.execute_reply.started":"2024-04-01T18:14:22.132184Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T18:52:48.276917Z","iopub.status.busy":"2024-04-01T18:52:48.275875Z","iopub.status.idle":"2024-04-01T18:52:48.281090Z","shell.execute_reply":"2024-04-01T18:52:48.280192Z","shell.execute_reply.started":"2024-04-01T18:52:48.276881Z"},"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append('/kaggle/input/sea-pix-gan-reproduction/pytorch/github/2/PyTorch')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T18:52:51.539323Z","iopub.status.busy":"2024-04-01T18:52:51.538865Z","iopub.status.idle":"2024-04-01T18:52:51.546659Z","shell.execute_reply":"2024-04-01T18:52:51.545419Z","shell.execute_reply.started":"2024-04-01T18:52:51.539287Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n"," > Training pipeline for Sea-pix-GAN models\n","   * Original paper: https://doi.org/10.1016/j.jvcir.2023.104021\n","\"\"\"\n","\n","# py libs\n","import os\n","import sys\n","import yaml\n","import argparse\n","from PIL import Image\n","# pytorch libs\n","import torch\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import torchvision.transforms as transforms\n","# local libs\n","from nets.seapixgan import SeaPixGan_Nets\n","from nets.commons import Weights_Normal\n","from utils.data_utils import GetTrainingPairs, GetValImage"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T18:52:52.422563Z","iopub.status.busy":"2024-04-01T18:52:52.422126Z","iopub.status.idle":"2024-04-01T18:52:52.440371Z","shell.execute_reply":"2024-04-01T18:52:52.439012Z","shell.execute_reply.started":"2024-04-01T18:52:52.422528Z"},"trusted":true},"outputs":[],"source":["# Clear any arguments that are already present (such as the notebook's name)\n","sys.argv = ['']\n","\n","# Now, add your argument list as if you were passing them to the command line\n","sys.argv += [\"--cfg_file\", \"/kaggle/input/sea-pix-gan-reproduction/pytorch/github/2/PyTorch/configs/train_euvp.yaml\"]\n","sys.argv += [\"--epoch\", \"0\"]\n","sys.argv += [\"--num_epochs\", \"201\"]\n","sys.argv += [\"--n_critic\", \"5\"]\n","\n","## get configs and training options\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--cfg_file\", type=str, default=\"configs/train_euvp.yaml\")\n","parser.add_argument(\"--epoch\", type=int, default=0, help=\"which epoch to start from\")\n","parser.add_argument(\"--num_epochs\", type=int, default=150, help=\"number of epochs of training\")\n","parser.add_argument(\"--n_critic\", type=int, default=5, help=\"training steps for D per iter w.r.t G\")\n","args = parser.parse_args()\n","\n","## training params\n","epoch = args.epoch\n","num_epochs = args.num_epochs\n","num_critic = args.n_critic\n","model_v = \"Sea-pix-GAN\" "]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T18:52:53.242085Z","iopub.status.busy":"2024-04-01T18:52:53.241251Z","iopub.status.idle":"2024-04-01T18:52:53.249753Z","shell.execute_reply":"2024-04-01T18:52:53.248200Z","shell.execute_reply.started":"2024-04-01T18:52:53.242029Z"},"trusted":true},"outputs":[],"source":["# load the data config file\n","with open(args.cfg_file) as f:\n","    cfg = yaml.load(f, Loader=yaml.FullLoader)\n","\n","# get info from config file\n","dataset_name = cfg[\"dataset_name\"] \n","dataset_path = cfg[\"dataset_path\"]\n","channels = cfg[\"chans\"]\n","img_width = cfg[\"im_width\"]\n","img_height = cfg[\"im_height\"] \n","val_interval = cfg[\"val_interval\"]\n","ckpt_interval = cfg[\"ckpt_interval\"]"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T18:52:54.233553Z","iopub.status.busy":"2024-04-01T18:52:54.232808Z","iopub.status.idle":"2024-04-01T18:52:54.239322Z","shell.execute_reply":"2024-04-01T18:52:54.238335Z","shell.execute_reply.started":"2024-04-01T18:52:54.233510Z"},"trusted":true},"outputs":[],"source":["## create dir for model and validation data\n","samples_dir = \"samples/%s/%s\" % (model_v, dataset_name)\n","checkpoint_dir = \"checkpoints/%s/%s/\" % (model_v, dataset_name)\n","os.makedirs(samples_dir, exist_ok=True)\n","os.makedirs(checkpoint_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-01T18:52:58.456156Z","iopub.status.busy":"2024-04-01T18:52:58.455752Z","iopub.status.idle":"2024-04-01T18:53:20.430442Z","shell.execute_reply":"2024-04-01T18:53:20.429244Z","shell.execute_reply.started":"2024-04-01T18:52:58.456130Z"},"trusted":true},"outputs":[],"source":["\"\"\" Sea-pix-GAN specifics: loss functions and specified hyperparams\n","-------------------------------------------------\"\"\"\n","L1_G  = torch.nn.L1Loss() # l1 loss term\n","L_BCE = torch.nn.BCELoss() # Binary cross entropy\n","lambda_1 = 100\n","batch_size = 64\n","lr = 2 * 10e-4\n","beta_1 = 0.5\n","beta_2 = 0.999 # not specified, use PyTorch default\n","\n","\n","# Initialize generator and discriminator\n","seapixgan_ = SeaPixGan_Nets(base_model='pix2pix')\n","generator = seapixgan_.netG\n","discriminator = seapixgan_.netD"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# see if cuda is available\n","if torch.cuda.is_available():\n","    generator = generator.cuda()\n","    discriminator = discriminator.cuda()\n","    L1_G = L1_G.cuda()\n","    L_BCE = L_BCE.cuda()\n","    Tensor = torch.cuda.FloatTensor\n","else:\n","    Tensor = torch.FloatTensor\n","\n","# Initialize weights or load pretrained models\n","if args.epoch == 0:\n","    generator.apply(Weights_Normal)\n","    discriminator.apply(Weights_Normal)\n","else:\n","    generator.load_state_dict(torch.load(\"checkpoints/%s/%s/generator_%d.pth\" % (model_v, dataset_name, args.epoch)))\n","    discriminator.load_state_dict(torch.load(\"checkpoints/%s/%s/discriminator_%d.pth\" % (model_v, dataset_name, epoch)))\n","    print (\"Loaded model from epoch %d\" %(epoch))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(beta_1, beta_2))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(beta_1, beta_2))\n","\n","## Data pipeline\n","# TODO: make sure preprocessing is correct\n","transforms_ = [\n","    transforms.Resize((img_height, img_width), Image.BICUBIC),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","]\n","\n","dataloader = DataLoader(\n","    GetTrainingPairs(dataset_path, dataset_name, transforms_=transforms_),\n","    batch_size = batch_size,\n","    shuffle = True,\n","    num_workers = 8,\n",")\n","\n","val_dataloader = DataLoader(\n","    GetValImage(dataset_path, dataset_name, transforms_=transforms_, sub_dir='validation'),\n","    batch_size=4,\n","    shuffle=True,\n","    num_workers=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## Training pipeline\n","for epoch in range(epoch, num_epochs):\n","    for i, batch in enumerate(dataloader):\n","        # Model inputs\n","        imgs_distorted = Variable(batch[\"A\"].type(Tensor)) # x: input underwater img\n","        imgs_good_gt = Variable(batch[\"B\"].type(Tensor)) # y: ground truth underwater img\n","\n","        ## Train Discriminator\n","        optimizer_D.zero_grad()\n","        \n","        imgs_fake = generator(imgs_distorted)\n","        pred_real = discriminator(imgs_good_gt, imgs_distorted)\n","        pred_fake = discriminator(imgs_fake, imgs_distorted)\n","        # ALL L_bce LOSSES WOULD BE BETTER IF THE SECOND\n","        # ARGUMENT IS MANUALLY PLACED (TENSOR SIZE OF IMAGE!)\n","        loss_D_gen = L_BCE(pred_fake, torch.zeros_like(pred_fake))\n","        loss_D_real = L_BCE(pred_real, torch.ones_like(pred_real))\n","        loss_D = loss_D_gen + loss_D_real\n","        loss_D.backward()\n","        optimizer_D.step()\n","\n","        ## Train Generator at 1:num_critic rate \n","        optimizer_G.zero_grad()\n","        if i % num_critic == 0:\n","            # regenerate imgs\n","            imgs_fake = generator(imgs_distorted)\n","            pred_fake = discriminator(imgs_fake.detach(), imgs_distorted.detach())\n","            # calculate loss function\n","            loss_1 = L1_G(imgs_fake, imgs_good_gt)\n","            loss_cgan = L_BCE(pred_fake, torch.ones_like(pred_fake))\n","            loss_G = loss_cgan + lambda_1 * loss_1 # Total loss: Eq.4 in paper\n","            # backward & steps\n","            loss_G.backward()\n","            optimizer_G.step()\n","\n","        ## Print log\n","        if not i%50:\n","            sys.stdout.write(\"\\r[Epoch %d/%d: batch %d/%d] [DLoss: %.3f, GLoss: %.3f]\"\n","                              %(\n","                                epoch, num_epochs, i, len(dataloader),\n","                                loss_D.item(), loss_G.item(),\n","                               )\n","            )\n","        ## If at sample interval save image\n","        batches_done = epoch * len(dataloader) + i\n","        if batches_done % val_interval == 0:\n","            imgs = next(iter(val_dataloader))\n","            imgs_val = Variable(imgs[\"val\"].type(Tensor))\n","            imgs_gen = generator(imgs_val)\n","            img_sample = torch.cat((imgs_val.data, imgs_gen.data), -2)\n","            save_image(img_sample, \"samples/%s/%s/%s.png\" % (model_v, dataset_name, batches_done), nrow=5, normalize=True)\n","\n","    ## Save model checkpoints\n","    if (epoch % ckpt_interval == 0):\n","        torch.save(generator.state_dict(), \"checkpoints/%s/%s/generator_%d.pth\" % (model_v, dataset_name, epoch))\n","        torch.save(discriminator.state_dict(), \"checkpoints/%s/%s/discriminator_%d.pth\" % (model_v, dataset_name, epoch))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"modelInstanceId":18475,"sourceId":22306,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
